from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras.preprocessing import image
import skimage.io as io
import matplotlib.pyplot as plt
import os

def unet(input_size):
    '''U-Netz Model'''
    '''Contracting Path:'''
    inputs = Input((input_size,input_size,1))
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)
     
    '''Expansive Path:'''
    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)

    model = Model(inputs = inputs, outputs = conv10)
    
    '''Definieren weiterer Hyperparameter'''
    model.compile(optimizer = Adam(lr = 1e-4), loss = "binary_crossentropy", metrics = ['accuracy'])
    return model


def data_numpy_array(image_folder, mask_folder, targetSize):
    image_arr = []
    mask_arr  = []
    
    '''Laden der FRS-Bilder'''
    
    img_list = os.listdir(image_folder)
    img_list = sorted(img_list)
  
    for item in img_list:
        img_path = os.path.join(image_folder, item)
        img = image.load_img(img_path, target_size=(targetSize,targetSize), grayscale=True)
        x = image.img_to_array(img)
        x = x/255
        x = np.expand_dims(x, axis=0)
        image_arr.append(x)
    
    '''Laden der dazugehörigen Masken'''
    
    mask_list = os.listdir(mask_folder)
    mask_list = sorted(mask_list)
   
    for item in mask_list:
        mask_path = os.path.join(mask_folder, item)
        mask = image.load_img(mask_path, target_size=(400,400), grayscale=True)
        x = image.img_to_array(mask)
        x=x/255
        x[x!=1] = 0 
        x = np.expand_dims(x, axis=0)
        mask_arr.append(x)
     
    image_arr = np.concatenate(image_arr, axis=0)   
    mask_arr = np.concatenate(mask_arr, axis=0)     
    
    '''Hinzufügen der bivariaten Normalverteilung'''
    
    output_Mask = [] 
    for mask in mask_arr: 
            '''Finden des Schwerpunktes der ärztlich eingezeichneten Punkt-Struktur'''
            
            m = mask[:,:,0]-1
            m = m*m
            m = m / np.sum(np.sum(m))
            dx = np.sum(m, 1)
            dy = np.sum(m, 0)
            cx = np.sum(dx * np.arange(400))/(400)
            cy = np.sum(dy * np.arange(400))/(400)
            '''Einfügen der bivariaten Normalverteilung der Aktivierungen um den Schwerpunkt'''  
                
            gauss_arr=np.zeros((targetSize,targetSize,1))
            for x in range(targetSize):
                for y in range(targetSize):
                    #Verschiebung der Verteilung
                    _x=x-cx*targetSize
                    _y=y-cy*targetSize
                    d = np.sqrt(_x*_x+_y*_y)
                    #Festlegen der Parameter, sigma bestimmt die Streuung der Aktivierungen
                    sigma, mu = 10.0, 0.0
                    gauss_arr[x,y,0] = np.exp(-( (d-mu)**2 / ( 2.0 * sigma**2 ) ) )
            gauss_arr = gauss_arr/ np.max(gauss_arr)                      
            output_Mask.append(gauss_arr)
           
    output_Mask=np.asarray(output_Mask) 
    return image_arr, output_Mask

'''Erzeugen von Trainings- und Validierungs-Datensätzen'''
print("Erzeugen des Trainings-Datensatzes...")
trainImg, trainMask = data_numpy_array('/home/constantin/NeuralNetworks/Classifier/RawImport/Train/Image',
                                 '/home/constantin/NeuralNetworks/Classifier/RawImport/Train/Mask',128)
print("Erzeugen des Validierungs-Datensatzes...")
validImg, validMask= data_numpy_array('/home/constantin/NeuralNetworks/Classifier/RawImport/Validation/Image',
                                '/home/constantin/NeuralNetworks/Classifier/RawImport/Validation/Mask',128)

print("Beispiel-Bild mit Heatmap:")
io.imshow(trainImg[0,:,:,0])
plt.show()
io.imshow(trainMask[0,:,:,0])
plt.show()

'''Definieren des U-Net-Models und Training'''

model = unet(128)
history = model.fit(trainImg,trainMask, epochs = 60,batch_size = 4, shuffle = True,validation_data = (validImg,validMask), verbose = 1)
